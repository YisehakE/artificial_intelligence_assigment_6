{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence\n",
    "# 464/664\n",
    "# Assignment #6\n",
    "\n",
    "## General Directions for this Assignment\n",
    "\n",
    "00. We're using a Jupyter Notebook environment (tutorial available here: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html),\n",
    "01. Read the entire notebook before beginning your work, \n",
    "02. Output format should be exactly as requested (it is your responsibility to make sure notebook looks as expected on Gradescope),\n",
    "03. Each helper function should be preceeded by documentation (Markdown cell), \n",
    "04. Each helper function (not `train`) should be followed by three assert-style unit tests,\n",
    "05. **Do not use any AI/ML libraries, packages, such as pandas, scikit (numpy is fine)**\n",
    "06. Functions should do only one thing,\n",
    "07. Check submission deadline on Gradescope, \n",
    "08. Rename the file to Last_First_assignment_6, \n",
    "09. Submit your notebook (as .ipynb, not PDF) using Gradescope, and\n",
    "10. Do not submit any other files.\n",
    "\n",
    "## Before You Submit...\n",
    "\n",
    "1. Re-read the general instructions provided above, and\n",
    "2. Hit \"Kernel\"->\"Restart & Run All\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For this assignment we will implement a Decision Tree using the ID3 Algorithm. The goal is classify a mushroom as either edible ('e') or poisonous ('p'). Dataset has been uploaded to Canvas. In case you'd like to learn more about it, here's the link to the repo: https://archive.ics.uci.edu/dataset/73/mushroom. \n",
    "\n",
    "\n",
    "Our  Decision Tree pipeline is as follows:\n",
    "\n",
    "\n",
    "1) `cross_validate` will take data (supplied as folds using 10 fold cross validation) and do the following:\n",
    "* For each setting of depth limit (the hyperparameter in decision trees, including 0)\n",
    "* * and for each fold of data\n",
    "* * * use `create_train_test` to split current fold into train and test\n",
    "* * * call `train` to build and return a decision tree, \n",
    "* * * call `classify` to use the tree to get classifications,\n",
    "* * * call `evaluate` to compare classifications to the actual answers (ground truth),\n",
    "* * * Print the performance for that fold\n",
    "* * Summarize the performance for that depth limit over all folds using `get_stats`\n",
    "\n",
    "\n",
    "2) `pretty_print_tree(tree)` will print what the tree looks like when using the **entire** data set (no train/test split) with depth limit set to None.\n",
    "\n",
    "\n",
    "All the code in this pipeline has been provided, except for a working `train` function. The `train` function currently returns a hard-coded tree from our lecture. Don't do that. Use ID3 to build your tree and use the depth limit to stop. When you're train function is complete, it should work for the lecture data, and mushrooms. Although `train` is terrible right now, pay attention to how the tree is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"note\"></a>\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Note</strong>\n",
    "    <p>\n",
    "        Let's start with our example from the 06-Nov lecture. Target variable is Safe?, which can be yes or no. Anything *_lecture refers to the dataset we walked through in class.  \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lecture = [['round','large','blue','no'],\n",
    "['square','large','green','yes'],\n",
    "['square','small','red','no'],\n",
    "['round','large','red','yes'],\n",
    "['square','small','blue','no'],\n",
    "['round','small','blue','no'],\n",
    "['round','small','red','yes'],\n",
    "['square','small','green','no'],\n",
    "['round','large','green','yes'],\n",
    "['square','large','green','yes'],\n",
    "['square','large','red','no'],\n",
    "['square','large','green','yes'],\n",
    "['round','large','red','yes'],\n",
    "['square','small','red','no'],\n",
    "['round','small','green','no']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_lecture[0]) # a record of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names_lecture = ['shape', \n",
    "                      'size', \n",
    "                      'color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_folds\"></a>\n",
    "## create_folds\n",
    "\n",
    "\n",
    "With n-fold cross validation, we divide our data set into n subgroups called \"folds\" and then use those folds for training and testing. For data set with 100 observations (or records), n set to 10 would have 10 observations in each fold.\n",
    "\n",
    "* **data** List: a list (data_lecture, for instance)\n",
    "* **n** int: number of folds\n",
    "\n",
    "\n",
    "**returns** \n",
    "folds, which is a list of n items, where each item is a list containing a subgroup of xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(data: List, n: int) -> List[List[List]]:\n",
    "    k, m = divmod(len(data), n)\n",
    "    return list(data[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_lecture = create_folds(data=data_lecture, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(folds_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(folds_lecture[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(folds_lecture[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create_train_test\"></a>\n",
    "## create_train_test\n",
    "\n",
    "\n",
    "This function takes the n folds and returns the train and test sets. One of the n folds is used to test, the others are used for training.\n",
    "\n",
    "* **folds** List[List[List]]: see `create_folds`\n",
    "* **index** int: fold index that is used for testing\n",
    "\n",
    "\n",
    "**returns** \n",
    "folds, which is a list of n items, where each item is a list containing a subgroup of xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds: List[List[List]], index: int) -> Tuple[List[List], List[List]]:\n",
    "    training = []\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = training + fold\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lecture, test_lecture = create_train_test(folds_lecture, 0) # test data is folds_lecture index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lecture, test_lecture = create_train_test(folds_lecture, 1) # test data is folds_lecture index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"note\"></a>\n",
    "\n",
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <p>\n",
    "        Let's load the mushroom data.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"parse_data\"></a>\n",
    "## parse_data\n",
    "\n",
    "Opens a file, splits on comma, and shuffles data before returning as a List of list. \n",
    "\n",
    "* **file_name** Str: filename for data\n",
    "\n",
    "\n",
    "**returns** \n",
    "Data as a list of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_name: str) -> List[List]:\n",
    "    data = []\n",
    "    file = open(file_name, \"r\")\n",
    "    for line in file:\n",
    "        datum = [value for value in line.rstrip().split(\",\")]\n",
    "        data.append(datum)\n",
    "    random.shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mushroom = parse_data(\"agaricus-lepiota.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <strong>Important</strong>\n",
    "    <p>\n",
    "        We're going to move the target column (mushroom edible or poisonous) to the last column to match the lecture's format, where Safe? was at the end.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mushroom = [record[1:]+[record[0]] for record in data_mushroom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_mushroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_mushroom[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names_mushroom = ['cap-shape',\n",
    "                   'cap-surface',\n",
    "                   'cap-color',\n",
    "                   'bruises?',\n",
    "                   'odor',\n",
    "                   'gill-attachment',\n",
    "                   'gill-spacing',\n",
    "                   'gill-size',\n",
    "                   'gill-color',\n",
    "                   'stalk-shape',\n",
    "                   'stalk-root',\n",
    "                   'stalk-surface-above-ring',\n",
    "                   'stalk-surface-below-ring',\n",
    "                   'stalk-color-above-ring',\n",
    "                   'stalk-color-below-ring',\n",
    "                   'veil-type',\n",
    "                   'veil-color',\n",
    "                   'ring-number',\n",
    "                   'ring-type',\n",
    "                   'spore-print-color',\n",
    "                   'population',\n",
    "                   'habitat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_answers\"></a>\n",
    "## get_answers\n",
    "\n",
    "This function extracts a list of the target values from data. The function assumes the target variable is the last column of the data.\n",
    "\n",
    "* **data** List[List]: The data provided in a list of list format identical to the structure of `data_lecture` or `data_mushroom`\n",
    "\n",
    "\n",
    "**returns** \n",
    "A list of the values of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(data):\n",
    "    return [record[-1] for record in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_answers([]) == []\n",
    "assert get_answers(data_lecture) == ['no', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'no', 'yes', 'yes', 'no', 'no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_mode\"></a>\n",
    "## get_mode\n",
    "\n",
    "This function finds the mode of a list of items. \n",
    "\n",
    "* **answers** List: A list of items\n",
    "\n",
    "**returns** \n",
    "The item that appears the most often in the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mode(answers):\n",
    "    count_dict = {}\n",
    "    for answer in answers:\n",
    "        if answer in count_dict:\n",
    "            count_dict[answer] = count_dict[answer] + 1\n",
    "        else:\n",
    "            count_dict[answer] = 1\n",
    "    mode_count = max(count_dict.values())\n",
    "    mode = [k for k, v in count_dict.items() if v == mode_count]\n",
    "    return mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_mode(['no', 'no', 'no', 'yes']) == 'no'\n",
    "assert get_mode(['no', 'no', 'yes', 'yes']) == 'no'\n",
    "assert get_mode(['no', 'yes', 'yes', 'yes']) == 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve_feature_choices_and_index_map\n",
    "Function that finds the possible choices for a given feature in context of dataset, along with a mapping between feature name and associated index based on attribute list.\n",
    "\n",
    "* **data** List[List[str]]: list of records that contain value for each feature in attribute names along with associated target value.\n",
    "* **attribute_names** List[str]: list of features, i.e attributes, defined for given dataset.\n",
    "\n",
    "\n",
    "* **RETURN** Tuple[Dict[str, Set[str]]], Dict[str, int]: yields a map from feature to possible choices, along with anohter map from feature to dataset column index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_feature_choices_and_index_map(data, attribute_names):\n",
    "  feature_choices = {}\n",
    "  feature_index_map = {}\n",
    "\n",
    "  for i, feature in enumerate(attribute_names):\n",
    "     feature_choices[feature] = set()\n",
    "     feature_index_map[feature] = i\n",
    "     \n",
    "  for col in range(len(data[0]) - 1):\n",
    "      feature = attribute_names[col]\n",
    "      for row in range(len(data)):\n",
    "         val = data[row][col]\n",
    "         feature_choices[feature].add(val)\n",
    "\n",
    "  return feature_choices, feature_index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat_choices_1, test_feature_index_map_1 = retrieve_feature_choices_and_index_map(data_lecture, attribute_names_lecture)\n",
    "\n",
    "assert(test_feat_choices_1 == {\"size\": {\"large\", \"small\"}, \"color\": {\"blue\", \"red\", \"green\"}, \"shape\": {\"round\", \"square\"}})\n",
    "assert(test_feature_index_map_1 == {\"shape\": 0, \"size\": 1, \"color\": 2})\n",
    "\n",
    "\n",
    "test_data_cats_humans = [\n",
    "                ['yes', 'no', 'no', 'human'],\n",
    "                ['no', 'yes', 'yes', 'cat'],\n",
    "                ['no', 'yes', 'no', 'human']\n",
    "\n",
    "                ]\n",
    "test_attr_names_cats_humans = ['intelligent?', 'lazy?', 'fur?']\n",
    "\n",
    "test_feat_choices_2, test_feature_index_map_2 = retrieve_feature_choices_and_index_map(test_data_cats_humans, test_attr_names_cats_humans)\n",
    "assert(test_feat_choices_2 == {\"intelligent?\": {\"yes\", \"no\"}, \"lazy?\": {\"yes\", \"no\"}, \"fur?\": {\"yes\", \"no\"}})\n",
    "assert(test_feature_index_map_2 == {\"intelligent?\": 0, \"lazy?\": 1, \"fur?\": 2})\n",
    "\n",
    "\n",
    "test_data_job_candidates = [\n",
    "                ['<=1', 'bachelors', 'no', 'decline'],\n",
    "                ['<=5', 'phd', 'yes', 'hire'],\n",
    "                ['<=10', 'masters', 'no', 'hire'],\n",
    "                ['<=1', 'masters', 'yes', 'hire']\n",
    "                ]\n",
    "test_attr_names_job_candidates = ['yearsExperience', 'education', 'requireSponsorship?']\n",
    "test_feat_choices_3, test_feature_index_map_3 = retrieve_feature_choices_and_index_map(test_data_job_candidates, test_attr_names_job_candidates)\n",
    "assert(test_feat_choices_3 == {\"yearsExperience\": {\"<=1\", \"<=5\", \"<=10\"}, \"education\": {\"bachelors\", \"masters\", \"phd\"}, \"requireSponsorship?\": {\"yes\", \"no\"}})\n",
    "assert(test_feature_index_map_3 == {\"yearsExperience\": 0, \"education\": 1, \"requireSponsorship?\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve_target_sizes\n",
    "\n",
    "Function that calculates the number of occurences for all possible target values that exist in dataset.\n",
    "\n",
    "* **data** List[List[str]]: list of records that contain value for each feature in attribute names along with associated target value.\n",
    "* **target_types** List[str]: list of all the possible target variable values.\n",
    "\n",
    "\n",
    "* **RETURN** Tuple[int, int]: yields the number of occurences for both target value types hat exist in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_target_sizes(data, target_types): # Only accounts for supervised learning classification with TWO types\n",
    "  target_sz_1, target_sz_2 = 0, 0\n",
    "  for record in data:\n",
    "        target_val = record[-1]\n",
    "        if target_val == target_types[0]: \n",
    "          target_sz_1 += 1\n",
    "        elif target_val == target_types[1]:\n",
    "          target_sz_2 += 1\n",
    "  return target_sz_1, target_sz_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_target_sz_1, test_1_target_sz_2 = retrieve_target_sizes(data_lecture, [\"yes\", \"no\"])\n",
    "assert(test_1_target_sz_1 == 7 and test_1_target_sz_2 ==8)\n",
    "\n",
    "test_data_cats_humans = [\n",
    "                ['yes', 'no', 'no', 'human'],\n",
    "                ['no', 'yes', 'yes', 'cat'],\n",
    "                ['no', 'yes', 'no', 'human']\n",
    "\n",
    "                ]\n",
    "test_targets_cats_humans = ['human', 'cat']\n",
    "\n",
    "test_2_target_sz_1, test_2_target_sz_2 = retrieve_target_sizes(test_data_cats_humans, test_targets_cats_humans)\n",
    "assert(test_2_target_sz_1 == 2 and test_2_target_sz_2 == 1)\n",
    "\n",
    "test_data_job_candidates = [\n",
    "                ['<=1', 'bachelors', 'no', 'decline'],\n",
    "                ['<=5', 'phd', 'yes', 'hire'],\n",
    "                ['<=10', 'masters', 'no', 'hire'],\n",
    "                ['<=1', 'masters', 'yes', 'hire']\n",
    "                ]\n",
    "test_targets_job_candidates = ['hire', 'decline']\n",
    "\n",
    "test_3_target_sz_1, test_3_target_sz_2 = retrieve_target_sizes(test_data_job_candidates, test_targets_job_candidates)\n",
    "assert(test_3_target_sz_1 == 3 and test_3_target_sz_2 == 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## entropy_hx\n",
    "Function that calculates the suprise in a dataset based on entropy equation H(x), utilizing the the number of target types in proportion to the size of dataset.\n",
    "* **target_sz_1** int: the number of occurences for one of target values in a dataset.\n",
    "* **target_sz_2** int: the number of occurences for the other target value in a dataset.\n",
    "* **proportion_denom** int:  size of dataset, typically denoted size as dataset that contains a specified choice for a feature.\n",
    "\n",
    "\n",
    "* **RETURN** float: returns the calculated entropy or suprise in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_hx(target_sz_1, target_sz_2, proportion_denom): # Only accounts for supervised learning classification with TWO types\n",
    "  if target_sz_1 + target_sz_2 != proportion_denom:\n",
    "    print(\"Error: proportions sum don't equare to 1\")\n",
    "    return None\n",
    "  \n",
    "  if target_sz_1 == 0 or target_sz_2 == 0: return 0.0\n",
    "  \n",
    "  p_1, p_2 = float(target_sz_1 / proportion_denom), float(target_sz_2 / proportion_denom)\n",
    "\n",
    "  target_entropy_1 = -p_1 * float(numpy.log2(p_1)) * p_1 \n",
    "  target_entropy_2 = -p_2 * float(numpy.log2(p_2)) * p_2\n",
    "\n",
    "  return target_entropy_1 + target_entropy_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion_denom = 8 # Test different proportions against a common denominator\n",
    "\n",
    "test_1_target_sz_1 = 4\n",
    "test_1_target_sz_2 = 4\n",
    "\n",
    "test_entropy_1 = entropy_hx(test_1_target_sz_1, test_1_target_sz_2, test_proportion_denom)\n",
    "assert(test_entropy_1 == 0.5)\n",
    "\n",
    "\n",
    "test_2_target_sz_1 = 8\n",
    "test_2_target_sz_2 = 0\n",
    "\n",
    "test_entropy_2 = entropy_hx(test_2_target_sz_1, test_2_target_sz_2, test_proportion_denom)\n",
    "assert(test_entropy_2 == 0.0)\n",
    "\n",
    "test_3_target_sz_1 = 2\n",
    "test_3_target_sz_2 = 6\n",
    "\n",
    "test_entropy_3 = entropy_hx(test_3_target_sz_1, test_3_target_sz_2, test_proportion_denom)\n",
    "assert(test_entropy_3 == 0.3584585933443496)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve_target_types\n",
    "Function that retrieves all the different types of target values.\n",
    "\n",
    "* **data** List[List[str]]: list of records that contain value for each feature in attribute names along with associated target value.\n",
    "\n",
    "* **RETURN** Tuple[List[str], Set[str]]: all the unique types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_target_types(data): # Only accounts for supervised learning classification with TWO types\n",
    "  answers = get_answers(data)\n",
    "  types = set() # Use set to disregard duplicated target values in data\n",
    "\n",
    "  for target_val in answers:\n",
    "    if len(types) == 2: break\n",
    "    types.add(target_val)\n",
    "\n",
    "  return types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_types_1 = retrieve_target_types(data_lecture)\n",
    "assert(test_types_1 == {\"yes\", \"no\"})\n",
    "\n",
    "test_types_2 = retrieve_target_types(data_mushroom)\n",
    "assert(test_types_2 == {\"p\", \"e\"})\n",
    "\n",
    "test_data_job_candidates = [\n",
    "                ['<=1', 'bachelors', 'no', 'decline'],\n",
    "                ['<=5', 'phd', 'yes', 'hire'],\n",
    "                ['<=10', 'masters', 'no', 'hire'],\n",
    "                ['<=1', 'masters', 'yes', 'hire']\n",
    "                ]\n",
    "\n",
    "test_types_3 = retrieve_target_types(test_data_job_candidates)\n",
    "assert(test_types_3 == {\"hire\", \"decline\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve_feature_choice_subset\n",
    "Function that generates a subset for a dataset for all records that contain the choice or rather the specified value for chosen feature.\n",
    "\n",
    "* **data** List[List[str]]: list of records that contain value for each feature in attribute names along with associated target value.\n",
    "* **choice** str: the specified value for a given feature.\n",
    "* **feature_col** int: column index marking the feature we're looking at in the dataset.\n",
    "\n",
    "\n",
    "* **RETURN** List[List[str]]: the of records that contain specified value for a chosen feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_feature_choice_subset(data, choice, feature_col):\n",
    "  subset = []\n",
    "  for record in data:\n",
    "    if record[feature_col] == choice:\n",
    "      subset.append(record)\n",
    "  \n",
    "  return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_choice_sub_1 = retrieve_feature_choice_subset(data_lecture, \"round\", 0)\n",
    "assert(test_feature_choice_sub_1 == [\n",
    "                                      ['round','large','blue','no'],\n",
    "                                      ['round','large','red','yes'],\n",
    "                                      ['round','small','blue','no'],\n",
    "                                      ['round','small','red','yes'],\n",
    "                                      ['round','large','green','yes'],\n",
    "                                      ['round','large','red','yes'],\n",
    "                                      ['round','small','green','no']\n",
    "                                    ]\n",
    "      )\n",
    "\n",
    "\n",
    "test_feature_choice_sub_2 = retrieve_feature_choice_subset(test_feature_choice_sub_1, \"large\", 1)\n",
    "\n",
    "assert(test_feature_choice_sub_2 == [\n",
    "                                      ['round','large','blue','no'],\n",
    "                                      ['round','large','red','yes'],\n",
    "                                      ['round','large','green','yes'],\n",
    "                                      ['round','large','red','yes'],\n",
    "                                    ]\n",
    "      )\n",
    "\n",
    "\n",
    "test_feature_choice_sub_3 = retrieve_feature_choice_subset(test_feature_choice_sub_2, \"red\", 2)\n",
    "assert(test_feature_choice_sub_3 == [\n",
    "                                      ['round','large','red','yes'],\n",
    "                                      ['round','large','red','yes'],\n",
    "                                    ]\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve_entropy_weighted_avg\n",
    "Function that calculates the weighted average for the feature entropy considering a given feature in the id3 run.\n",
    "\n",
    "* **choice_entropy_map** Dict[str, Tuple(float, int)]: dictionary that maps a choice and it's calculated entropy in a subset along with the subset size with containing that choice.\n",
    "* **baseline_sz** int: the size of the dataset before considering any choice for a feature we're considering at moment of id3 run.\n",
    "\n",
    "\n",
    "* **RETURN** float: the weighted average among all the choice entropies when considering a given feature in the id3 run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_entropy_weighted_avg(choice_entropy_map, baseline_sz):\n",
    "  entropy_weight_avg = 0.0\n",
    "\n",
    "  for choice in choice_entropy_map:\n",
    "    entropy, choice_sz = choice_entropy_map[choice]\n",
    "    weight = float(choice_sz / baseline_sz) \n",
    "    entropy_weight_avg += float(weight * entropy)\n",
    "    \n",
    "  return entropy_weight_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_baseline_sz = 8\n",
    "test_choice_entropy_map_1 = {\"small\": (1.0, 8), \"large\": (0.0, 8)}   \n",
    "\n",
    "test_weighted_entropy_1 = retrieve_entropy_weighted_avg(test_choice_entropy_map_1, test_baseline_sz)\n",
    "assert(test_weighted_entropy_1 == 1.0)\n",
    "\n",
    "\n",
    "test_choice_entropy_map_2 = {\"small\": (0.5, 4), \"large\": (0.5, 4)}   \n",
    "test_weighted_entropy_2 = retrieve_entropy_weighted_avg(test_choice_entropy_map_2, test_baseline_sz)\n",
    "\n",
    "assert(test_weighted_entropy_2 == 0.5)\n",
    "\n",
    "test_choice_entropy_map_2 = {\"small\": (0.5, 4), \"large\": (0.5, 2)}   \n",
    "test_weighted_entropy_2 = retrieve_entropy_weighted_avg(test_choice_entropy_map_2, test_baseline_sz)\n",
    "\n",
    "assert(test_weighted_entropy_2 == 0.375)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrive_best_feature\n",
    "* **subset** List[List[str]]: a list of relevant records constrained to  prior selected features and corresponding choices.\n",
    "* **feature_choices** Dict[str, Set[str]]: a map with pairs of features pointing to its possible choices.\n",
    "* **feature_index_map** Dict[str, int]: a map with pairs of features pointing to it's correseponding dataset column index.\n",
    "* **remaining_feats** Set[str]: collection of remaining attributes to find best feature from.\n",
    "* **target_types** List[str]: collection of the unique types of target variables in subset.\n",
    "* **baseline_sz** int: the size of the subset evaluated from prior feature choice(s) before examining remaining features.\n",
    "* **baseline_entropy** float: the entropy of the subset constrained to feature choice(s) chosen before examining remaining features.\n",
    "\n",
    "* **RETURN** Tuple[str, Dict[str, Tuple[float, float]]]: the next best feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_best_feature(subset, feature_choices, feature_index_map, remaining_feats, target_types, baseline_sz, baseline_entropy):\n",
    "  max_info_gain, best_feat = float(\"-inf\"), None\n",
    "\n",
    "  for feature in remaining_feats: \n",
    "    choice_entropy_map = {}\n",
    "\n",
    "    for choice in feature_choices[feature]:\n",
    "      # Get partition of subset that contains current feature's choice\n",
    "      subset_with_feat_choice = retrieve_feature_choice_subset(subset, choice, feature_col=feature_index_map[feature]) \n",
    "      choice_sz = len(subset_with_feat_choice)\n",
    "\n",
    "      # Count up the size for both types of target values per choice made for a feature\n",
    "      target_sz_1, target_sz_2 = retrieve_target_sizes(subset_with_feat_choice, target_types)\n",
    "      \n",
    "      # Calculate entropy per choice made for a feature\n",
    "      choice_entropy = entropy_hx(target_sz_1, target_sz_2, choice_sz)\n",
    "      choice_entropy_map[choice] = (choice_entropy, choice_sz)\n",
    "    \n",
    "    feature_entropy = retrieve_entropy_weighted_avg(choice_entropy_map, baseline_sz)\n",
    "    feature_info_gain = baseline_entropy - feature_entropy\n",
    "\n",
    "    if feature_info_gain > max_info_gain:\n",
    "      max_info_gain = feature_info_gain\n",
    "      best_feat = feature\n",
    "\n",
    "  return best_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Corresponds with \"Lecture 12: Decision trees\" example with lecture data '''\n",
    "# General arguments for all three tests\n",
    "test_feature_choices, test_feature_index_map = retrieve_feature_choices_and_index_map(data_lecture, attribute_names_lecture)\n",
    "test_attr_name_set = set(attribute_names_lecture)\n",
    "test_target_types = list(retrieve_target_types(data_lecture))\n",
    "\n",
    "test_subset_1 = data_lecture\n",
    "test_baseline_sz_1 = 15\n",
    "test_baseline_entropy_1 = 0.9968\n",
    "\n",
    "\n",
    "\n",
    "test_best_feature_1 = retrive_best_feature(test_subset_1,\n",
    "                                            test_feature_choices, \n",
    "                                            test_feature_index_map, \n",
    "                                            test_attr_name_set, \n",
    "                                            test_target_types, \n",
    "                                            test_baseline_sz_1, \n",
    "                                            test_baseline_entropy_1)\n",
    "\n",
    "assert(test_best_feature_1 == \"size\")\n",
    "test_attr_name_set.remove(test_best_feature_1)\n",
    "\n",
    "test_subset_2 = retrieve_feature_choice_subset(test_subset_1, \"large\", 1)\n",
    "test_baseline_sz_2 = 8\n",
    "test_baseline_entropy_2 = 0.811\n",
    "\n",
    "\n",
    "test_best_feature_2 = retrive_best_feature(test_subset_2,\n",
    "                                           test_feature_choices, \n",
    "                                           test_feature_index_map, \n",
    "                                           test_attr_name_set, \n",
    "                                           test_target_types, \n",
    "                                           test_baseline_sz_2, \n",
    "                                           test_baseline_entropy_2)\n",
    "\n",
    "assert(test_best_feature_2 == \"color\")\n",
    "test_attr_name_set.remove(test_best_feature_2)\n",
    "\n",
    "test_best_feature_3 = list(test_attr_name_set)[0]\n",
    "assert(test_best_feature_3 == \"shape\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## train\n",
    "\n",
    "This function takes training_data, attribute names, and the depth limit and returns the decision tree as a nested dictionary. If the depth is 0, a dictionary is not returned. Instead, the mode of the target values is returned (i.e., majority class). \n",
    "\n",
    "* **training_data** List[List]: The data\n",
    "* **attribute_names** List: The attribute names of the data (22 for mushroom; size, shape, and color for the lecture)\n",
    "* **depth_limit** int: The depth limit of the tree\n",
    "\n",
    "\n",
    "**returns** \n",
    "* **dt** Dict: The trained decision tree using the ID3 algorithm (entropy, information gain). It is represented as a nested dictionary. The dictionary returned for the lecture is structured as below:\n",
    "\n",
    "```\n",
    "{\n",
    "('size', 1, 'large'): \n",
    "    {('color', 2, 'blue'): 'no', \n",
    "     ('color', 2, 'green'): 'yes', \n",
    "     ('color', 2, 'red'): \n",
    "         {('shape', 0, 'round'): 'yes', \n",
    "          ('shape', 0, 'square'): 'no'}\n",
    "     }, \n",
    "('size', 1, 'small'): \n",
    "     {('shape', 0, 'square'): 'no', \n",
    "      ('shape', 0, 'round'): \n",
    "          {('color', 2, 'blue'): 'no', \n",
    "           ('color', 2, 'red'): 'yes', \n",
    "           ('color', 2, 'green'): 'no'}\n",
    "      }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Notice that the keys are tuples; for instance, ('size', 1, 'large') is a key. The key includes the attribute's name, column number in data, and value.\n",
    "\n",
    "\n",
    "The function currently returns a hard-coded tree. Your implementation should replace this with a tree that is learned from the data using the ID3 algorithm. You do not have to assert test `train`, but it may be worthwhile to check that it can return the tree from the lecture once your implementation is in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, attribute_names, depth_limit=None):\n",
    "  feature_choices, feature_index_map = retrieve_feature_choices_and_index_map(training_data, attribute_names)\n",
    "  target_types_set = retrieve_target_types(training_data)\n",
    "  attr_names_set, tar_types_list = set(attribute_names), list(target_types_set)\n",
    "  default_label = get_mode(get_answers(training_data))\n",
    "\n",
    "  def id3(subset, feature_choices, feature_index_map, remaining_feats, target_types, default, depth_limit):  \n",
    "    if len(subset) == 0: return default # Base case: no more records to build tree from\n",
    "\n",
    "    baseline_sz = len(subset)\n",
    "    target_sz_1, target_sz_2 = retrieve_target_sizes(subset, target_types)\n",
    "    baseline_entropy = entropy_hx(target_sz_1, target_sz_2, baseline_sz)\n",
    "\n",
    "    if baseline_entropy == 0: return subset[0][-1]  # Base case: subset is homogenous \n",
    "\n",
    "    default_label = get_mode(get_answers(subset))\n",
    "    if len(remaining_feats) == 0: return default_label # Base case: no more other features to split tree from\n",
    "\n",
    "    if depth_limit and depth_limit == 0: return default_label\n",
    "    if depth_limit and depth_limit > 0: depth_limit -= 1\n",
    "\n",
    "    # Determine best feature to follow up from previous subset constrained to prior feature choice\n",
    "    best_feat = retrive_best_feature(subset, feature_choices, feature_index_map, remaining_feats, target_types, baseline_sz, baseline_entropy)\n",
    "\n",
    "    # This is where we recursively the tree, branching out from subtree to best feature\n",
    "    next_node = {}\n",
    "    next_best_feat_attr = remaining_feats.copy()\n",
    "    next_best_feat_attr.remove(best_feat)\n",
    "\n",
    "    for choice in feature_choices[best_feat]: # Go through the choices for next best feature, i.e domain\n",
    "      subset_with_feat_choice = retrieve_feature_choice_subset(subset, choice, feature_index_map[best_feat]) \n",
    "\n",
    "      feat_branch = (best_feat, feature_index_map[best_feat], choice)\n",
    "      child =  id3(subset=subset_with_feat_choice, \n",
    "                   feature_choices=feature_choices, \n",
    "                   feature_index_map=feature_index_map, \n",
    "                   remaining_feats=next_best_feat_attr, \n",
    "                   target_types=target_types, \n",
    "                   default=default_label,\n",
    "                   depth_limit=depth_limit\n",
    "                  )\n",
    "      next_node[feat_branch] = child\n",
    "    return next_node\n",
    "  \n",
    "  dt = id3(subset=training_data, \n",
    "           feature_choices=feature_choices, \n",
    "           feature_index_map=feature_index_map, \n",
    "           remaining_feats=attr_names_set, \n",
    "           target_types=tar_types_list, \n",
    "           default=default_label,\n",
    "           depth_limit=depth_limit)\n",
    "  \n",
    "  return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_lecture = train(training_data=train_lecture, attribute_names=attribute_names_lecture, depth_limit=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_prediction\"></a>\n",
    "## get_prediction\n",
    "\n",
    "This recursive function uses a decision tree represented as a nested dictionary get a prediction from a record, which is a row of the data. \n",
    "\n",
    "* **record** List[]: A row of data to be predicted\n",
    "* **dt** the decision tree used to make the prediction\n",
    "\n",
    "\n",
    "**returns** \n",
    "A prediction ('yes' or 'no' for instance, from our Self Check example.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(record, dt):\n",
    "    if not isinstance(dt, dict):\n",
    "        return dt\n",
    "    else:\n",
    "        for key, value in dt.items():\n",
    "            if record[key[1]]==key[2]:\n",
    "                return get_prediction(record, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_prediction(['round','large','blue','no'], dt=dt_lecture))\n",
    "print(get_prediction(['square','large','green','yes'], dt=dt_lecture))\n",
    "print(get_prediction(['square','small','red','no'], dt=dt_lecture))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"classify\"></a>\n",
    "## classify\n",
    "\n",
    "This function takes a decision tree, observations, and a labeled flag to return a list of classifications. \n",
    "\n",
    "* **dt** Dict: The decision tree as a nested dictionary\n",
    "* **observation** List[List]: a list of items, where each item is a row of the data\n",
    "* **labeled** Bool: true for labeled data\n",
    "\n",
    "\n",
    "**returns** \n",
    "* **y_hat** List: A list of classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(dt, observations):\n",
    "    y_hat = []\n",
    "    for record in observations:\n",
    "        y_hat.append(get_prediction(record, dt))   \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classify(dt=dt_lecture, observations=test_lecture))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluate\"></a>\n",
    "## evaluate\n",
    "\n",
    "This function evaluates the performance of a classifier. It takes a data set (training set or test set) and the classification result (see [classify](#classify) above and calculates the classification error rate:\n",
    "\n",
    "$$error\\_rate=\\frac{errors}{n}$$ \n",
    "\n",
    "* **y_hat** List: A list of predictions\n",
    "* **observations** List[List]: Data to be predicted (typically training or test set)\n",
    "\n",
    "\n",
    "**returns** \n",
    "\n",
    "* **error_rate** float: The error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_hat, observations):\n",
    "    errors = 0\n",
    "    ground_truth = get_answers(observations)\n",
    "    for index in range(len(y_hat)):\n",
    "        if y_hat[index] != ground_truth[index]:\n",
    "            errors = errors + 1\n",
    "    return errors / (len(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(evaluate(classify(dt=dt_lecture, observations=data_lecture), observations=data_lecture))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get_stats\"></a>\n",
    "## get_stats\n",
    "\n",
    "This function computes the mean and the standard deviation for a given list of observations. \n",
    "\n",
    "* **observations** List[float]: A list of observations\n",
    "\n",
    "\n",
    "**returns** (mean, standard deviation) Tuple[float,float]: tuple consisting of mean and the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(observations: List[float]) -> Tuple[float,float]:\n",
    "    mean = sum(observations) / len(observations)\n",
    "    variance = sum([(elem - mean)**2 for elem in observations]) / len(observations)\n",
    "    std_dev = math.sqrt(variance)\n",
    "    return mean, std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_stats([2, 4, 4, 4, 5, 5, 7, 9]) == (5.0, 2.0)\n",
    "assert get_stats([1, 1, 1]) == (1.0, 0.0)\n",
    "assert get_stats([0]) == (0.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cross_validate\"></a>\n",
    "## cross_validate\n",
    "\n",
    "This function takes folds of data to `train`, `classify`, and `evaluate`.\n",
    "\n",
    "\n",
    "* **folds** List[List[List]]: The original dataset partitioned into folds (see `create_folds` above)\n",
    "* **attribute_names** int: the feature names\n",
    "* **hyperparameters** List: A list of hyperparameters to explore (depth limits for a decision tree, for instance)\n",
    "\n",
    "**returns** \n",
    "\n",
    "Nothing is returned, but for each hyperparameter setting, the function prints out the fold number and the error rate for that fold. The mean and variance is printed across folds for each hyperparameter setting. The error rates are reported in terms of percents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(folds, attribute_names, hyperparameters):\n",
    "    for hyperparameter in hyperparameters:\n",
    "        train_error, test_error  = [], []\n",
    "        error_list_train, error_list_test = [], []\n",
    "        for fold_index in range(len(folds)):\n",
    "            training_data, test_data = create_train_test(folds, fold_index)\n",
    "            tree = train(training_data=training_data, attribute_names=attribute_names, depth_limit=hyperparameter)\n",
    "            y_hat_train = classify(tree, training_data)\n",
    "            y_hat_test = classify(tree, test_data)\n",
    "            error_rate_train = evaluate(y_hat_train, training_data)\n",
    "            error_rate_test = evaluate(y_hat_test, test_data)\n",
    "            error_list_train.append(error_rate_train)\n",
    "            error_list_test.append(error_rate_test)\n",
    "            print(f\"Fold: {fold_index}\\tTrain Error: {error_rate_train*100:.2f}%\\tTest Error: {error_rate_test*100:.2f}%\")\n",
    "        print(f\"***\")\n",
    "        print(f\"Depth limit: {hyperparameter}\")\n",
    "        print(f\"\\nMean(Std. Dev.) over all folds:\\n-------------------------------\")\n",
    "        print(f\"Train Error: {get_stats(error_list_train)[0]*100:.2f}%({get_stats(error_list_train)[1]*100:.2f}%) Test Error: {get_stats(error_list_test)[0]*100:.2f}%({get_stats(error_list_test)[1]*100:.2f}%)\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cross_validate(folds=folds_lecture, attribute_names=attribute_names_lecture, hyperparameters=[0, 1, 2, 3, 4, 5, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pretty_print_tree\"></a>\n",
    "## pretty_print_tree\n",
    "\n",
    "This function provides a text-based representation of a decision tree that is represented as a nested dictionary. \n",
    "\n",
    "* **dt** Dict: The decision tree as a nested dictionary\n",
    "* **tab_space** Int: How much to tab successive depth levels of the resulting tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_tree(dt, tab_space):\n",
    "    for key, value in dt.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(\"  \" * tab_space + str(key[0]).upper() + \" - \" + str(key[2]) + \": \")\n",
    "            print(\"\\n\")\n",
    "            pretty_print_tree(value, tab_space+3)\n",
    "        else:\n",
    "            print(\"  \" * tab_space + str(key[0]).upper() + \" - \" + str(key[2]) + \" =====> \" + str(value))\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_lecture = train(training_data=data_lecture, attribute_names=attribute_names_lecture, depth_limit=None)\n",
    "pretty_print_tree(dt_lecture, tab_space=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <p>\n",
    "        Let's work on the mushroom data. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the Mushrooom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "folds_mushroom = create_folds(data=data_mushroom, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 5\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 6\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 7\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 8\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 9\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "***\n",
      "Depth limit: 4\n",
      "\n",
      "Mean(Std. Dev.) over all folds:\n",
      "-------------------------------\n",
      "Train Error: 0.00%(0.00%) Test Error: 0.00%(0.00%)\n",
      "\n",
      "\n",
      "Fold: 0\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 1\tTrain Error: 0.00%\tTest Error: 0.00%\n",
      "Fold: 2\tTrain Error: 0.00%\tTest Error: 0.00%\n"
     ]
    }
   ],
   "source": [
    "cross_validate(folds=folds_mushroom, attribute_names=attribute_names_mushroom, hyperparameters=[0, 1, 2, 3, 4, 5, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: lemonchiffon; margin:20px; padding: 20px;\">\n",
    "    <p>\n",
    "        Let's work on the mushroom data. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the Mushroom Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_mushroom = train(training_data=data_mushroom, attribute_names=attribute_names_mushroom, depth_limit=None)\n",
    "pretty_print_tree(dt_mushroom, tab_space=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Submit...\n",
    "\n",
    "1. Re-read the general instructions provided above, and\n",
    "2. Hit \"Kernel\"->\"Restart & Run All\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
